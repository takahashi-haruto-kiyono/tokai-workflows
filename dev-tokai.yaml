# CDP Data Pipeline - Phase 2 Version
# 担当: 高橋
# 目的: GCS inbox → L0 → archive → Dataform L1変換
#
# Phase 2: Dataform L1変換を追加

main:
  params: [event]
  steps:
    # ============================================
    # Step 1: イベントデータからファイル情報を抽出
    # ============================================
    - extract_file_info:
        assign:
          - bucket: ${event.data.bucket}
          - file_name: ${event.data.name}
          - file_path: '${"gs://" + bucket + "/" + file_name}'
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - dataset_l0: "l0_cdp_raw"
          - dataform_repository: '${"projects/" + project_id + "/locations/asia-northeast1/repositories/data-preparation"}'
          - dataform_workspace: "prd"

    # ============================================
    # Step 2: inbox配下のファイルかチェック
    # パス例: raw/l0_D100_test/inbox/D100_test_20251021.csv
    # ============================================
    - validate_inbox_path:
        switch:
          - condition: ${not text.match_regex(file_name, ".*\\/inbox\\/.*")}
            steps:
              - log_skip:
                  call: sys.log
                  args:
                    data: '${"SKIPPED: File is not in inbox directory: " + file_name}'
                    severity: INFO
              - return_skip:
                  return:
                    status: "SKIPPED"
                    reason: "File is not in inbox directory"
                    file_path: ${file_path}

    # ============================================
    # Step 3: ファイル名からテーブルIDを抽出
    # パス例: raw/D005_DM_response_flag/inbox/D005_20251026.csv
    #         → folder_name = "D005_DM_response_flag"
    #         → table_id = "D005" (アンダースコアで分割して最初)
    # ============================================
    - extract_table_id:
        assign:
          - file_parts: ${text.split(file_name, "/")}
          - folder_name: ${file_parts[1]}
          - table_id_from_path: ${text.split(folder_name, "_")[0]}
          - base_file_name: ${file_parts[len(file_parts)-1]}
          - table_id_from_file: ${text.split(base_file_name, "_")[0]}

    # ============================================
    # Step 4: テーブルIDの一致確認
    # ============================================
    - validate_table_id:
        switch:
          - condition: ${table_id_from_path != table_id_from_file}
            steps:
              - log_table_id_mismatch:
                  call: sys.log
                  args:
                    json:
                      error_type: "TABLE_ID_MISMATCH"
                      error_message: "Table ID mismatch"
                      file_path: ${file_path}
                      table_id_from_path: ${table_id_from_path}
                      table_id_from_file: ${table_id_from_file}
                      folder_name: ${folder_name}
                      base_file_name: ${base_file_name}
                    severity: ERROR
              - raise_table_id_error:
                  raise:
                    message: '${"Table ID mismatch: path=" + table_id_from_path + " file=" + table_id_from_file}'

    - get_table_config:
        assign:
          - table_id: ${table_id_from_path}

    # ============================================
    # Step 5: BigQueryからテーブル設定を取得
    # ============================================
    - query_table_config:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${project_id}
          body:
            query: |
              SELECT
                table_id,
                table_name,
                load_type,
                skip_leading_rows,
                field_delimiter,
                allow_quoted_newlines,
                allow_jagged_rows,
                max_bad_records
              FROM `dev_cdp_config.table_mapping`
              WHERE table_id = @table_id
            parameterMode: "NAMED"
            queryParameters:
              - name: "table_id"
                parameterType:
                  type: "STRING"
                parameterValue:
                  value: ${table_id}
            useLegacySql: false
        result: query_result

    # ============================================
    # Step 6: 設定が見つからない場合はエラー
    # ============================================
    - validate_config:
        switch:
          - condition: ${len(query_result.rows) == 0}
            steps:
              - log_config_not_found:
                  call: sys.log
                  args:
                    json:
                      error_type: "CONFIG_NOT_FOUND"
                      error_message: "No configuration found in table_mapping"
                      table_id: ${table_id}
                      file_path: ${file_path}
                      dataset: "dev_cdp_config"
                      table: "table_mapping"
                    severity: ERROR
              - raise_config_error:
                  raise:
                    message: '${"No configuration found for table_id: " + table_id}'

    - parse_config:
        assign:
          - config: ${query_result.rows[0].f}
          - table_name: ${config[1].v}
          - load_type: ${config[2].v}
          - skip_leading_rows: ${int(config[3].v)}
          - field_delimiter: ${config[4].v}
          - allow_quoted_newlines: ${config[5].v == "true"}
          - allow_jagged_rows: ${config[6].v == "true"}
          - max_bad_records: ${int(config[7].v)}

    # ============================================
    # Step 6.5: アーカイブパスを事前に計算 (ロード前)
    # ============================================
    - prepare_archive_path:
        assign:
          - timestamp: ${time.format(sys.now())}
          - timestamp_clean: '${text.replace_all(text.replace_all(text.replace_all(timestamp, "-", ""), ":", ""), "T", "")}'
          - timestamp_formatted: '${text.substring(timestamp_clean, 0, 14)}'
          - archive_path: '${text.replace_all(file_name, "/inbox/", "/archive/")}'
          - archive_path_with_ts: '${text.replace_all(archive_path, ".csv", "_" + timestamp_formatted + ".csv")}'

    # ============================================
    # Step 7: ファイルをアーカイブにコピー (ロード前に実行)
    # エラーハンドリング付き
    # ============================================
    - copy_to_archive_before_load:
        try:
          call: http.post
          args:
            url: '${"https://storage.googleapis.com/storage/v1/b/" + bucket + "/o/" + text.url_encode(file_name) + "/copyTo/b/" + bucket + "/o/" + text.url_encode(archive_path_with_ts)}'
            auth:
              type: OAuth2
          result: copy_result
        except:
          as: e
          steps:
            - log_copy_error:
                call: sys.log
                args:
                  json:
                    error_type: "FILE_COPY_FAILED"
                    error_message: "Failed to copy file to archive"
                    source_file: ${file_path}
                    destination_file: '${"gs://" + bucket + "/" + archive_path_with_ts}'
                    bucket: ${bucket}
                    error_details: ${e}
                  severity: ERROR
            - return_copy_failed:
                return:
                  status: "FAILED"
                  reason: "File copy failed (file may not exist)"
                  error: ${e}

    - log_archive_success:
        call: sys.log
        args:
          data: '${"File archived to: " + archive_path_with_ts}'
          severity: INFO

    # ============================================
    # Step 9: L0への読み込み
    # ============================================
    - load_to_l0:
        call: googleapis.bigquery.v2.jobs.insert
        args:
          projectId: ${project_id}
          body:
            configuration:
              load:
                sourceUris: ['${file_path}']
                destinationTable:
                  projectId: ${project_id}
                  datasetId: ${dataset_l0}
                  tableId: ${table_name}
                writeDisposition: "WRITE_APPEND"
                sourceFormat: "CSV"
                skipLeadingRows: ${skip_leading_rows}
                fieldDelimiter: ${field_delimiter}
                allowQuotedNewlines: ${allow_quoted_newlines}
                allowJaggedRows: ${allow_jagged_rows}
                maxBadRecords: ${max_bad_records}
                autodetect: false
                timePartitioning:
                  type: "DAY"
              jobTimeoutMs: 3600000
        result: load_job

    # ============================================
    # Step 10: L0ロードジョブの完了待機
    # ============================================
    - init_wait_loop:
        assign:
          - max_iterations: 720
          - iteration: 0

    - wait_for_l0_load:
        call: googleapis.bigquery.v2.jobs.get
        args:
          projectId: ${project_id}
          jobId: ${load_job.jobReference.jobId}
          location: ${load_job.jobReference.location}
        result: job_status

    - check_l0_load_status:
        switch:
          - condition: ${job_status.status.state == "DONE"}
            next: verify_l0_load_success
          - condition: ${iteration >= max_iterations}
            steps:
              - log_timeout:
                  call: sys.log
                  args:
                    json:
                      error_type: "LOAD_JOB_TIMEOUT"
                      error_message: "Load job timeout after 1 hour"
                      job_id: ${load_job.jobReference.jobId}
                      table_name: ${table_name}
                      file_path: ${file_path}
                      max_iterations: ${max_iterations}
                    severity: ERROR
              - raise_timeout:
                  raise:
                    message: "Load job timeout after 1 hour"
          - condition: ${job_status.status.state == "RUNNING" or job_status.status.state == "PENDING"}
            next: increment_iteration

    - increment_iteration:
        assign:
          - iteration: ${iteration + 1}
        next: sleep_before_retry_l0

    - sleep_before_retry_l0:
        call: sys.sleep
        args:
          seconds: 5
        next: wait_for_l0_load

    - verify_l0_load_success:
        switch:
          - condition: ${"errorResult" in job_status.status}
            steps:
              - log_load_failure:
                  call: sys.log
                  args:
                    json:
                      error_type: "BIGQUERY_LOAD_FAILED"
                      error_message: ${job_status.status.errorResult.message}
                      job_id: ${load_job.jobReference.jobId}
                      table_name: ${table_name}
                      file_path: ${file_path}
                      error_result: ${job_status.status.errorResult}
                    severity: ERROR
              - raise_load_error:
                  raise:
                    message: '${"L0 load failed: " + job_status.status.errorResult.message}'

    - log_l0_success:
        call: sys.log
        args:
          data: '${"L0 load completed successfully for " + table_name + ", job_id: " + load_job.jobReference.jobId}'
          severity: INFO

    # ============================================
    # Step 11: 元ファイルの削除
    # ============================================
    - delete_original_file:
        try:
          call: http.delete
          args:
            url: '${"https://storage.googleapis.com/storage/v1/b/" + bucket + "/o/" + text.url_encode(file_name)}'
            auth:
              type: OAuth2
        except:
          as: e
          steps:
            - log_delete_error:
                call: sys.log
                args:
                  json:
                    error_type: "FILE_DELETE_FAILED"
                    error_message: "Failed to delete original file"
                    file_path: ${file_path}
                    error_details: ${e}
                  severity: WARNING

    - log_delete_success:
        call: sys.log
        args:
          data: '${"Original file deleted: " + file_name}'
          severity: INFO

    # ============================================
    # Step 12: Dataform Compilation Result作成
    # ============================================
    - create_compilation_result:
        call: http.post
        args:
          url: '${"https://dataform.googleapis.com/v1beta1/" + dataform_repository + "/compilationResults"}'
          auth:
            type: OAuth2
          body:
            workspace: '${dataform_repository + "/workspaces/" + dataform_workspace}'
        result: compilation_result

    - log_compilation_success:
        call: sys.log
        args:
          data: "Dataform compilation result created"
          severity: INFO

    # ============================================
    # Step 13: Dataform Workflow Invocation実行
    # ============================================
    - create_workflow_invocation:
        call: http.post
        args:
          url: '${"https://dataform.googleapis.com/v1beta1/" + dataform_repository + "/workflowInvocations"}'
          auth:
            type: OAuth2
          body:
            compilationResult: ${compilation_result.body.name}
            invocationConfig:
              includedTags:
                - '${"l1_" + table_id}'
              transitiveDependenciesIncluded: true
        result: workflow_invocation

    - log_workflow_invocation_created:
        call: sys.log
        args:
          data: '${"Dataform workflow invocation created for table_id: " + table_id}'
          severity: INFO

    # ============================================
    # Step 14: Dataform完了待機
    # ============================================
    - wait_for_dataform_completion:
        try:
          call: waitForDataformCompletion
          args:
            workflow_name: ${workflow_invocation.body.name}
          result: dataform_status
        except:
          as: e
          steps:
            - log_dataform_error:
                call: sys.log
                args:
                  json:
                    error_type: "DATAFORM_EXECUTION_FAILED"
                    error_message: "Dataform execution failed"
                    table_id: ${table_id}
                    error_details: ${e}
                  severity: ERROR
            - raise_dataform_error:
                raise: ${e}

    - log_dataform_success:
        call: sys.log
        args:
          data: '${"Dataform L1 transformation completed successfully for table_id: " + table_id}'
          severity: INFO

    # ============================================
    # Step 15: 正常終了
    # ============================================
    - return_success:
        return:
          status: "SUCCESS"
          phase: "Phase 2 - L0 load and L1 transformation"
          table_id: ${table_id}
          table_name: ${table_name}
          load_type: ${load_type}
          file_path: ${file_path}
          archive_path: '${"gs://" + bucket + "/" + archive_path_with_ts}'
          l0_job_id: ${load_job.jobReference.jobId}
          dataform_invocation_name: ${workflow_invocation.body.name}
          dataform_status: ${dataform_status}

# ============================================
# サブワークフロー: Dataform完了待機
# ============================================
waitForDataformCompletion:
  params:
    - workflow_name
  steps:
    - check_dataform_job:
        call: http.get
        args:
          url: '${"https://dataform.googleapis.com/v1beta1/" + workflow_name}'
          auth:
            type: OAuth2
        result: job_status

    - check_if_done:
        switch:
          - condition: ${job_status.body.state == "SUCCEEDED"}
            return: ${job_status.body.state}
          - condition: ${job_status.body.state == "FAILED"}
            steps:
              - log_failure:
                  call: sys.log
                  args:
                    json:
                      error_type: "DATAFORM_FAILED"
                      job_status: ${job_status.body}
                    severity: ERROR
              - raise_failure:
                  raise: ${job_status.body.compilationResult}

    - wait_before_retry:
        call: sys.sleep
        args:
          seconds: 20
        next: check_dataform_job
