# CDP Data Pipeline - Phase 1 Version
# 担当: 高橋
# 目的: GCS inbox → L0 → archive（Dataform実行なし）
#
# Phase 1: L0ロードとアーカイブのみ実装
# Phase 2: 中西さんのDataform実装後に追加

main:
  params: [event]
  steps:
    # ============================================
    # Step 1: イベントデータからファイル情報を抽出
    # ============================================
    - extract_file_info:
        assign:
          - bucket: ${event.data.bucket}
          - file_name: ${event.data.name}
          - file_path: '${"gs://" + bucket + "/" + file_name}'
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - dataset_l0: "cdp_l0"
          - dataset_l1: "cdp_l1"

    # ============================================
    # Step 2: inbox配下のファイルかチェック
    # パス例: raw/D002_customer_master/inbox/D002_20250121.csv
    # ============================================
    - validate_inbox_path:
        switch:
          - condition: ${not text.match_regex(file_name, "/inbox/")}
            return:
              status: "SKIPPED"
              reason: "File is not in inbox directory"
              file_path: ${file_path}

    # ============================================
    # Step 3: ファイル名からテーブルIDを抽出
    # パス例: raw/D002_customer_master/inbox/D002_20250121.csv
    #         → file_parts[1] = "D002_customer_master"
    #         → table_id = "D002" (アンダースコアで分割して最初)
    # ============================================
    - extract_table_id:
        assign:
          - file_parts: ${text.split(file_name, "/")}
          - folder_name: ${file_parts[1]}
          - table_id_from_path: ${text.split(folder_name, "_")[0]}
          - base_file_name: ${file_parts[len(file_parts)-1]}
          - table_id_from_file: ${text.split(base_file_name, "_")[0]}

    # ============================================
    # Step 4: テーブルIDの一致確認
    # ============================================
    - validate_table_id:
        switch:
          - condition: ${table_id_from_path != table_id_from_file}
            raise:
              message: '${"Table ID mismatch: path=" + table_id_from_path + " file=" + table_id_from_file}'

    - get_table_config:
        assign:
          - table_id: ${table_id_from_path}

    # ============================================
    # Step 5: BigQueryからテーブル設定を取得
    # ============================================
    - query_table_config:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${project_id}
          body:
            query: |
              SELECT
                table_id,
                table_name,
                load_type,
                write_disposition,
                skip_leading_rows,
                field_delimiter,
                allow_quoted_newlines,
                allow_jagged_rows,
                max_bad_records
              FROM `cdp_config.table_mapping`
              WHERE table_id = @table_id
            parameterMode: "NAMED"
            queryParameters:
              - name: "table_id"
                parameterType:
                  type: "STRING"
                parameterValue:
                  value: ${table_id}
            useLegacySql: false
        result: query_result

    # ============================================
    # Step 6: 設定が見つからない場合はエラー
    # ============================================
    - validate_config:
        switch:
          - condition: ${len(query_result.rows) == 0}
            raise:
              message: '${"No configuration found for table_id: " + table_id}'

    - parse_config:
        assign:
          - config: ${query_result.rows[0].f}
          - table_name: ${config[1].v}
          - load_type: ${config[2].v}
          - write_disposition: ${config[3].v}
          - skip_leading_rows: ${int(config[4].v)}
          - field_delimiter: ${config[5].v}
          - allow_quoted_newlines: ${config[6].v == "true"}
          - allow_jagged_rows: ${config[7].v == "true"}
          - max_bad_records: ${int(config[8].v)}

    # ============================================
    # Step 7: L0への読み込み
    # ============================================
    - load_to_l0:
        call: googleapis.bigquery.v2.jobs.insert
        args:
          projectId: ${project_id}
          body:
            configuration:
              load:
                sourceUris: [${file_path}]
                destinationTable:
                  projectId: ${project_id}
                  datasetId: ${dataset_l0}
                  tableId: ${table_name}
                writeDisposition: ${write_disposition}
                sourceFormat: "CSV"
                skipLeadingRows: ${skip_leading_rows}
                fieldDelimiter: ${field_delimiter}
                allowQuotedNewlines: ${allow_quoted_newlines}
                allowJaggedRows: ${allow_jagged_rows}
                maxBadRecords: ${max_bad_records}
                autodetect: false
                timePartitioning:
                  type: "DAY"
                  field: "_loaded_at"
                schema:
                  fields: []
              jobTimeoutMs: 3600000  # 1時間タイムアウト
        result: load_job

    # ============================================
    # Step 8: L0ロードジョブの完了待機
    # ============================================
    - init_wait_loop:
        assign:
          - max_iterations: 720  # 720回 × 5秒 = 1時間
          - iteration: 0

    - wait_for_l0_load:
        call: googleapis.bigquery.v2.jobs.get
        args:
          projectId: ${project_id}
          jobId: ${load_job.jobReference.jobId}
          location: ${load_job.jobReference.location}
        result: job_status

    - check_l0_load_status:
        switch:
          - condition: ${job_status.status.state == "DONE"}
            next: verify_l0_load_success
          - condition: ${iteration >= max_iterations}
            raise:
              message: "Load job timeout after 1 hour"
          - condition: ${job_status.status.state == "RUNNING" or job_status.status.state == "PENDING"}
            next: increment_iteration

    - increment_iteration:
        assign:
          - iteration: ${iteration + 1}
        next: sleep_before_retry_l0

    - sleep_before_retry_l0:
        call: sys.sleep
        args:
          seconds: 5
        next: wait_for_l0_load

    - verify_l0_load_success:
        switch:
          - condition: ${"errorResult" in job_status.status}
            raise:
              message: '${"L0 load failed: " + job_status.status.errorResult.message}'

    - log_l0_success:
        call: sys.log
        args:
          data: '${"L0 load completed successfully for " + table_name + ", job_id: " + load_job.jobReference.jobId}'
          severity: INFO
        next: archive_file

    # ============================================
    # NOTE: Dataform実行はPhase 2で追加予定
    # 中西さんのDataform実装完了後に有効化
    # ============================================

    # ============================================
    # Step 9: 成功時のファイルアーカイブ
    # 変換例: raw/D002_customer_master/inbox/D002_20250121.csv
    #       → raw/D002_customer_master/archive/D002_20250121_20250121143022.csv
    # ============================================
    - archive_file:
        assign:
          - timestamp: ${time.format(sys.now())}
          - timestamp_formatted: '${text.replace_all(text.replace_all(timestamp, ":", ""), "-", "")}'
          - archive_path: '${text.replace_all(file_name, "/inbox/", "/archive/")}'
          - archive_path_with_ts: '${text.replace_all(archive_path, ".csv", "_" + text.substring(timestamp_formatted, 0, 14) + ".csv")}'

    - log_archive_start:
        call: sys.log
        args:
          data: '${"Archiving file to: " + archive_path_with_ts}'
          severity: INFO

    - copy_to_archive:
        call: googleapis.storage.v1.objects.copy
        args:
          sourceBucket: ${bucket}
          sourceObject: ${file_name}
          destinationBucket: ${bucket}
          destinationObject: ${archive_path_with_ts}
        result: copy_result

    # ============================================
    # Step 10: 元ファイルの削除
    # ============================================
    - delete_original_file:
        call: googleapis.storage.v1.objects.delete
        args:
          bucket: ${bucket}
          object: ${file_name}

    - log_delete_success:
        call: sys.log
        args:
          data: '${"Original file deleted: " + file_name}'
          severity: INFO

    # ============================================
    # Step 11: 正常終了
    # ============================================
    - return_success:
        return:
          status: "SUCCESS"
          phase: "Phase 1 - L0 load only"
          table_id: ${table_id}
          table_name: ${table_name}
          load_type: ${load_type}
          file_path: ${file_path}
          archive_path: '${"gs://" + bucket + "/" + archive_path_with_ts}'
          l0_job_id: ${load_job.jobReference.jobId}
          note: "Dataform L1 transformation will be added in Phase 2"
