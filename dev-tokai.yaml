# CDP Data Pipeline - Phase 1 Version
# 担当: 高橋
# 目的: GCS inbox → L0 → archive（Dataform実行なし）
#
# Phase 1: L0ロードとアーカイブのみ実装
# Phase 2: 中西さんのDataform実装後に追加

main:
  params: [event]
  steps:
    # ============================================
    # Step 1: デバッグ - event の中身を全部ログ出力
    # ============================================
    - debug_event_structure:
        call: sys.log
        args:
          data: '${"DEBUG EVENT STRUCTURE: " + json.encode_to_string(event)}'
          severity: INFO

    # ============================================
    # Step 2: イベントデータからファイル情報を抽出
    # ============================================
    - extract_file_info:
        assign:
          - bucket: ${event.data.bucket}
          - file_name: ${event.data.name}
          - file_path: '${"gs://" + bucket + "/" + file_name}'
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - dataset_l0: "dev_cdp_config"
          - dataset_l1: "cdp_l1"

    - debug_file_info:
        call: sys.log
        args:
          data: '${"DEBUG: bucket=" + bucket + ", file_name=" + file_name + ", file_path=" + file_path}'
          severity: INFO

    # ============================================
    # Step 3: inbox配下のファイルかチェック
    # パス例: raw/l0_D100_test/inbox/D100_test_20251021.csv
    # ============================================
    - validate_inbox_path:
        switch:
          - condition: ${not text.match_regex(file_name, ".*\\/inbox\\/.*")}
            steps:
              - log_skip:
                  call: sys.log
                  args:
                    data: '${"SKIPPED: File is not in inbox directory: " + file_name}'
                    severity: INFO
              - return_skip:
                  return:
                    status: "SKIPPED"
                    reason: "File is not in inbox directory"
                    file_path: ${file_path}

    # ============================================
    # Step 3: ファイル名からテーブルIDを抽出
    # パス例: raw/l0_D100_test/inbox/D100_test_20251021.csv
    #         → folder_name = "l0_D100_test"
    #         → folder_name_no_prefix = "D100_test" (l0_ を削除)
    #         → table_id = "D100" (アンダースコアで分割して最初)
    # ============================================
    - extract_table_id:
        assign:
          - file_parts: ${text.split(file_name, "/")}
          - folder_name: ${file_parts[1]}
          - folder_name_no_prefix: ${text.replace_all(folder_name, "l0_", "")}
          - table_id_from_path: ${text.split(folder_name_no_prefix, "_")[0]}
          - base_file_name: ${file_parts[len(file_parts)-1]}
          - table_id_from_file: ${text.split(base_file_name, "_")[0]}

    # ============================================
    # Step 4: テーブルIDの一致確認
    # ============================================
    - validate_table_id:
        switch:
          - condition: ${table_id_from_path != table_id_from_file}
            raise:
              message: '${"Table ID mismatch: path=" + table_id_from_path + " file=" + table_id_from_file}'

    - get_table_config:
        assign:
          - table_id: ${table_id_from_path}

    # ============================================
    # Step 5: BigQueryからテーブル設定を取得
    # ============================================
    - query_table_config:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: ${project_id}
          body:
            query: |
              SELECT
                table_id,
                table_name,
                load_type,
                write_disposition,
                skip_leading_rows,
                field_delimiter,
                allow_quoted_newlines,
                allow_jagged_rows,
                max_bad_records
              FROM `dev_cdp_config.table_mapping`
              WHERE table_id = @table_id
            parameterMode: "NAMED"
            queryParameters:
              - name: "table_id"
                parameterType:
                  type: "STRING"
                parameterValue:
                  value: ${table_id}
            useLegacySql: false
        result: query_result

    # ============================================
    # Step 6: 設定が見つからない場合はエラー
    # ============================================
    - validate_config:
        switch:
          - condition: ${len(query_result.rows) == 0}
            raise:
              message: '${"No configuration found for table_id: " + table_id}'

    - parse_config:
        assign:
          - config: ${query_result.rows[0].f}
          - table_name: ${config[1].v}
          - load_type: ${config[2].v}
          - write_disposition: ${config[3].v}
          - skip_leading_rows: ${int(config[4].v)}
          - field_delimiter: ${config[5].v}
          - allow_quoted_newlines: ${config[6].v == "true"}
          - allow_jagged_rows: ${config[7].v == "true"}
          - max_bad_records: ${int(config[8].v)}

    # ============================================
    # Step 6.5: アーカイブパスを事前に計算 (ロード前)
    # ============================================
    - prepare_archive_path:
        assign:
          - timestamp: ${time.format(sys.now())}
          - timestamp_clean: '${text.replace_all(text.replace_all(text.replace_all(timestamp, "-", ""), ":", ""), "T", "")}'
          - timestamp_formatted: '${text.substring(timestamp_clean, 0, 14)}'
          - archive_path: '${text.replace_all(file_name, "/inbox/", "/archive/")}'
          - archive_path_with_ts: '${text.replace_all(archive_path, ".csv", "_" + timestamp_formatted + ".csv")}'

    # ============================================
    # Step 7: ファイルをアーカイブにコピー (ロード前に実行)
    # エラーハンドリング付き
    # ============================================
    - debug_copy_params:
        call: sys.log
        args:
          data: '${"DEBUG COPY: sourceBucket=" + bucket + ", sourceObject=" + file_name + ", destinationBucket=" + bucket + ", destinationObject=" + archive_path_with_ts}'
          severity: INFO

    - copy_to_archive_before_load:
        try:
          call: http.post
          args:
            url: '${"https://storage.googleapis.com/storage/v1/b/" + bucket + "/o/" + text.url_encode(file_name) + "/copyTo/b/" + bucket + "/o/" + text.url_encode(archive_path_with_ts)}'
            auth:
              type: OAuth2
          result: copy_result
        except:
          as: e
          steps:
            - log_copy_error:
                call: sys.log
                args:
                  data: '${"ERROR: Failed to copy file - " + json.encode_to_string(e)}'
                  severity: ERROR
            - return_copy_failed:
                return:
                  status: "FAILED"
                  reason: "File copy failed (file may not exist)"
                  error: ${e}

    - log_archive_success:
        call: sys.log
        args:
          data: '${"File archived to: " + archive_path_with_ts}'
          severity: INFO

    # ============================================
    # Step 9: L0への読み込み
    # ============================================
    - load_to_l0:
        call: googleapis.bigquery.v2.jobs.insert
        args:
          projectId: ${project_id}
          body:
            configuration:
              load:
                sourceUris: ['${file_path}']
                destinationTable:
                  projectId: ${project_id}
                  datasetId: ${dataset_l0}
                  tableId: ${table_name}
                writeDisposition: ${write_disposition}
                sourceFormat: "CSV"
                skipLeadingRows: ${skip_leading_rows}
                fieldDelimiter: ${field_delimiter}
                allowQuotedNewlines: ${allow_quoted_newlines}
                allowJaggedRows: ${allow_jagged_rows}
                maxBadRecords: ${max_bad_records}
                autodetect: true
                timePartitioning:
                  type: "DAY"
              jobTimeoutMs: 3600000
        result: load_job

    # ============================================
    # Step 10: L0ロードジョブの完了待機
    # ============================================
    - init_wait_loop:
        assign:
          - max_iterations: 720
          - iteration: 0

    - wait_for_l0_load:
        call: googleapis.bigquery.v2.jobs.get
        args:
          projectId: ${project_id}
          jobId: ${load_job.jobReference.jobId}
          location: ${load_job.jobReference.location}
        result: job_status

    - check_l0_load_status:
        switch:
          - condition: ${job_status.status.state == "DONE"}
            next: verify_l0_load_success
          - condition: ${iteration >= max_iterations}
            raise:
              message: "Load job timeout after 1 hour"
          - condition: ${job_status.status.state == "RUNNING" or job_status.status.state == "PENDING"}
            next: increment_iteration

    - increment_iteration:
        assign:
          - iteration: ${iteration + 1}
        next: sleep_before_retry_l0

    - sleep_before_retry_l0:
        call: sys.sleep
        args:
          seconds: 5
        next: wait_for_l0_load

    - verify_l0_load_success:
        switch:
          - condition: ${"errorResult" in job_status.status}
            raise:
              message: '${"L0 load failed: " + job_status.status.errorResult.message}'

    - log_l0_success:
        call: sys.log
        args:
          data: '${"L0 load completed successfully for " + table_name + ", job_id: " + load_job.jobReference.jobId}'
          severity: INFO

    # ============================================
    # Step 11: 元ファイルの削除
    # ============================================
    - delete_original_file:
        try:
          call: googleapis.storage.v1.objects.delete
          args:
            bucket: ${bucket}
            object: ${file_name}
        except:
          as: e
          steps:
            - log_delete_error:
                call: sys.log
                args:
                  data: '${"Failed to delete original file (may already be deleted): " + file_name + ", error: " + json.encode_to_string(e)}'
                  severity: WARNING

    - log_delete_success:
        call: sys.log
        args:
          data: '${"Original file deleted: " + file_name}'
          severity: INFO

    # ============================================
    # Step 12: 正常終了
    # ============================================
    - return_success:
        return:
          status: "SUCCESS"
          phase: "Phase 1 - L0 load only"
          table_id: ${table_id}
          table_name: ${table_name}
          load_type: ${load_type}
          file_path: ${file_path}
          archive_path: '${"gs://" + bucket + "/" + archive_path_with_ts}'
          l0_job_id: ${load_job.jobReference.jobId}
          note: "Dataform L1 transformation will be added in Phase 2"
